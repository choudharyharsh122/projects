{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import autograd.numpy.random as npr\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from autograd.core import primitive\n",
    "#import tensorflow as tf\n",
    "import math as mat\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# import wandb\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        # self.energy_out = nn.Sequential(\n",
    "        #                     nn.Linear(24, 1),nn.ReLU()\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh(x)\n",
    "        y = self.fc4(x)\n",
    "       \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "n_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip():\n",
    "    x_ip = 2 * torch.rand((50, 1)) - 1\n",
    "    #x_ip = torch.from_numpy(x.reshape(1, -1))\n",
    "    #print(type(x_ip))\n",
    "    #print(x_ip)\n",
    "    train_x = data_utils.TensorDataset(x_ip)\n",
    "    #print(type(train_x))\n",
    "    #train_x.size()\n",
    "    train_dataloader = data_utils.DataLoader(train_x, batch_size=50)\n",
    "    #print(len(train_dataloader))\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_Loss(X, model, res_loss):\n",
    "    X.requires_grad = True\n",
    "    y = model(X)\n",
    "    dy_dx = torch.autograd.grad(y, X, create_graph=True)[0]\n",
    "    return res_loss(y, -2*X*dy_dx)\n",
    "    #return ((y + 2*X*dy_dx)**2)/torch.Tensor(X.size()[0])\n",
    "\n",
    "def get_bc_loss(X_0, model, bc_loss):\n",
    "    y_0 = model(X_0)\n",
    "    return bc_loss(y_0, torch.Tensor(1))\n",
    "    #return (y_0 - torch.Tensor(1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_loader, model, optimizer, t, res_loss, bc_loss):\n",
    "    accumulated_res_loss = 0\n",
    "    accumulated_bc_loss = 0\n",
    "    accumulated_loss = 0\n",
    "    grad = []\n",
    "    #print(x_loader[0])\n",
    "    for i, X_in in enumerate(x_loader):\n",
    "        X = X_in[0]\n",
    "        print(X)\n",
    "        y = model(X)\n",
    "        #print(X)\n",
    "        #print(len(X[0]))\n",
    "        optimizer.zero_grad()\n",
    "        res_loss = get_res_Loss(X, model, res_loss)\n",
    "        X.requires_grad = False\n",
    "        bc_loss = get_bc_loss(torch.Tensor(0), model, bc_loss)\n",
    "        net_loss = res_loss + bc_loss\n",
    "        accumulated_res_loss += res_loss.item()\n",
    "        accumulated_bc_loss += bc_loss.item()\n",
    "        accumulated_loss += net_loss.item()\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "        if t==50 or t==100:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    grad.append(tuple(name, param.data, param.grad))\n",
    "                    #print(name, param.data, param.grad)\n",
    "        print(\"Residual Loss: \",res_loss)\n",
    "        print(\"Boundary Loss: \",bc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc4): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "1\n",
      "1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "tensor([[-0.3812],\n",
      "        [ 0.2399],\n",
      "        [-0.9540],\n",
      "        [-0.2627],\n",
      "        [ 0.1546],\n",
      "        [ 0.4265],\n",
      "        [-0.7386],\n",
      "        [ 0.6072],\n",
      "        [-0.5136],\n",
      "        [-0.4792],\n",
      "        [ 0.0809],\n",
      "        [ 0.1115],\n",
      "        [ 0.7821],\n",
      "        [ 0.4215],\n",
      "        [ 0.2888],\n",
      "        [-0.5216],\n",
      "        [ 0.0520],\n",
      "        [ 0.3619],\n",
      "        [ 0.0352],\n",
      "        [ 0.8024],\n",
      "        [-0.0642],\n",
      "        [ 0.7471],\n",
      "        [-0.3829],\n",
      "        [-0.6750],\n",
      "        [ 0.5459],\n",
      "        [ 0.0413],\n",
      "        [-0.0799],\n",
      "        [ 0.2635],\n",
      "        [ 0.7998],\n",
      "        [ 0.1640],\n",
      "        [-0.9436],\n",
      "        [-0.3589],\n",
      "        [-0.8417],\n",
      "        [ 0.8485],\n",
      "        [ 0.4086],\n",
      "        [-0.0520],\n",
      "        [-0.5869],\n",
      "        [ 0.1237],\n",
      "        [-0.1362],\n",
      "        [ 0.5056],\n",
      "        [ 0.0809],\n",
      "        [-0.7233],\n",
      "        [ 0.9156],\n",
      "        [-0.9397],\n",
      "        [ 0.5991],\n",
      "        [ 0.2236],\n",
      "        [ 0.4629],\n",
      "        [ 0.4288],\n",
      "        [-0.7219],\n",
      "        [-0.3165]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\KIT_data\\pickleFile\\GradFlow_PhyInf.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(x_loader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train(x_loader, model, optimizer, t, res_loss, bc_loss)\n",
      "\u001b[1;32md:\\KIT_data\\pickleFile\\GradFlow_PhyInf.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_loader, model, optimizer, t, res_loss, bc_loss)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#print(X)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#print(len(X[0]))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m res_loss \u001b[39m=\u001b[39m get_res_Loss(X, model, res_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m bc_loss \u001b[39m=\u001b[39m get_bc_loss(torch\u001b[39m.\u001b[39mTensor(\u001b[39m0\u001b[39m), model, bc_loss)\n",
      "\u001b[1;32md:\\KIT_data\\pickleFile\\GradFlow_PhyInf.ipynb Cell 7\u001b[0m in \u001b[0;36mget_res_Loss\u001b[1;34m(X, model, res_loss)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dy_dx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(y, X, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res_loss(y, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mX\u001b[39m*\u001b[39mdy_dx)\n",
      "File \u001b[1;32mc:\\Users\\choud\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:288\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    283\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 288\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\choud\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay=3e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                    factor=0.1, patience=30, threshold=0.0001, \n",
    "                                                    threshold_mode='rel', cooldown=0, min_lr=1e-15, eps=1e-08)\n",
    "#reg_loss =  torch.nn.MSELoss(beta=0.3)\n",
    "res_loss = torch.nn.MSELoss()\n",
    "bc_loss = torch.nn.MSELoss()\n",
    "#class_loss = torch.nn.CrossEntropyLoss()\n",
    "#ener_loss = torch.nn.L1Loss()\n",
    "loss_weight  = 0.5\n",
    "\n",
    "\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # wandb.log({\"Epoch Number \": t})\n",
    "    x_loader = get_ip()\n",
    "    print(len(x_loader))\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(x_loader, model, optimizer, t, res_loss, bc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 1D numpy array\n",
    "array_1d = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Convert the array to a 2D column vector\n",
    "array_2d = array_1d.reshape(-1, 1)\n",
    "\n",
    "print(array_2d)\n",
    "# Output:\n",
    "# [[1]\n",
    "#  [2]\n",
    "#  [3]\n",
    "#  [4]\n",
    "#  [5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 0, Batch 0, Loss 0.15152686834335327\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 1, Batch 0, Loss 0.14795473217964172\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 2, Batch 0, Loss 0.1445627510547638\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 3, Batch 0, Loss 0.14134174585342407\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 4, Batch 0, Loss 0.13828307390213013\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 5, Batch 0, Loss 0.13537845015525818\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 6, Batch 0, Loss 0.13262011110782623\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 7, Batch 0, Loss 0.13000060617923737\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 8, Batch 0, Loss 0.1275128722190857\n",
      "tensor([[0.3433],\n",
      "        [0.8054],\n",
      "        [0.4142],\n",
      "        [0.9477],\n",
      "        [0.3647],\n",
      "        [0.7008],\n",
      "        [0.8965],\n",
      "        [0.9300],\n",
      "        [0.9958],\n",
      "        [0.1340],\n",
      "        [0.3135],\n",
      "        [0.9227],\n",
      "        [0.5449],\n",
      "        [0.1141],\n",
      "        [0.5702],\n",
      "        [0.4080],\n",
      "        [0.1542],\n",
      "        [0.9494],\n",
      "        [0.3340],\n",
      "        [0.0465],\n",
      "        [0.3173],\n",
      "        [0.5327],\n",
      "        [0.4176],\n",
      "        [0.4184],\n",
      "        [0.8406],\n",
      "        [0.0121],\n",
      "        [0.1980],\n",
      "        [0.7205],\n",
      "        [0.2852],\n",
      "        [0.4537],\n",
      "        [0.2398],\n",
      "        [0.9759],\n",
      "        [0.9849],\n",
      "        [0.9663],\n",
      "        [0.4826],\n",
      "        [0.7567],\n",
      "        [0.2375],\n",
      "        [0.3807],\n",
      "        [0.2406],\n",
      "        [0.5806],\n",
      "        [0.5064],\n",
      "        [0.9124],\n",
      "        [0.1270],\n",
      "        [0.6383],\n",
      "        [0.4818],\n",
      "        [0.2831],\n",
      "        [0.8548],\n",
      "        [0.5012],\n",
      "        [0.2299],\n",
      "        [0.3760]])\n",
      "Epoch 9, Batch 0, Loss 0.12515026330947876\n",
      "Predictions:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generate random input data with shape (50, 1)\n",
    "X = torch.rand((50, 1))\n",
    "\n",
    "# Generate random target data with shape (50, 1)\n",
    "y = torch.rand((50, 1))\n",
    "\n",
    "# Create a TensorDataset object from the input and target data\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Create a DataLoader object with the dataset and a batch size of 50\n",
    "batch_size = 50\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Define a simple linear model\n",
    "\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Create an instance of the linear model\n",
    "model = LinearModel()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Get the input and target data for the current batch\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        # Zero out the gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        print(X_batch)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print some information about the training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss {loss.item()}\")\n",
    "\n",
    "# Make predictions on the input data\n",
    "y_pred = model(X)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\")\n",
    "#print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4115],\n",
      "        [ 1.5635],\n",
      "        [ 0.0667],\n",
      "        [-1.1132],\n",
      "        [-1.0782],\n",
      "        [ 1.3676],\n",
      "        [ 0.2273],\n",
      "        [ 0.0687],\n",
      "        [ 0.1842],\n",
      "        [-0.1622],\n",
      "        [-0.8458],\n",
      "        [ 0.9342],\n",
      "        [ 1.2204],\n",
      "        [-0.9464],\n",
      "        [-2.3768],\n",
      "        [ 0.7282],\n",
      "        [-0.4659],\n",
      "        [-0.1449],\n",
      "        [ 1.0596],\n",
      "        [-0.4627],\n",
      "        [ 0.0107],\n",
      "        [ 0.2685],\n",
      "        [ 0.6195],\n",
      "        [ 0.8953],\n",
      "        [ 0.4070],\n",
      "        [ 0.6639],\n",
      "        [ 0.5228],\n",
      "        [-1.4160],\n",
      "        [ 0.1051],\n",
      "        [ 0.0090],\n",
      "        [ 0.7763],\n",
      "        [-0.2247],\n",
      "        [ 0.7152],\n",
      "        [ 0.5923],\n",
      "        [ 1.2651],\n",
      "        [-1.7205],\n",
      "        [-1.3492],\n",
      "        [-0.8178],\n",
      "        [ 0.7622],\n",
      "        [-0.1610],\n",
      "        [ 0.8663],\n",
      "        [ 1.0093],\n",
      "        [ 0.6537],\n",
      "        [-0.9483],\n",
      "        [-0.4426],\n",
      "        [-0.4773],\n",
      "        [-0.9341],\n",
      "        [ 1.1678],\n",
      "        [-1.7881],\n",
      "        [ 0.0415]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\KIT_data\\pickleFile\\GradFlow_PhyInf.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#print(X)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Compute the gradient of y with respect to X\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m dy_dx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(y[\u001b[39m0\u001b[39;49m], X[\u001b[39m0\u001b[39;49m], create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Compute the derivative of the output with respect to the input\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#d2y_dx2 = torch.autograd.grad(dy_dx, X)[0]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Print the result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KIT_data/pickleFile/GradFlow_PhyInf.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(dy_dx)\n",
      "File \u001b[1;32mc:\\Users\\choud\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a toy neural network with a single input and output\n",
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "# Generate random input data with shape (batch_size, 1)\n",
    "batch_size = 50\n",
    "X = torch.randn((batch_size, 1))\n",
    "\n",
    "# Set X to require gradients\n",
    "X.requires_grad = True\n",
    "\n",
    "# Compute the output of the neural network\n",
    "y = model(X)\n",
    "print(y)\n",
    "#print(X)\n",
    "\n",
    "# Compute the gradient of y with respect to X\n",
    "dy_dx = torch.autograd.grad(y[0], X[0], create_graph=True)\n",
    "\n",
    "# Compute the derivative of the output with respect to the input\n",
    "#d2y_dx2 = torch.autograd.grad(dy_dx, X)[0]\n",
    "\n",
    "# Print the result\n",
    "print(dy_dx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
